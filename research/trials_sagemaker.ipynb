{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # Check Sklearn version\n",
    "sklearn.__version__\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "import datetime\n",
    "import time\n",
    "import tarfile\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "bucket = 'sagemaker-tutorials-mlhub' # Mention the created S3 bucket name here\n",
    "print(\"Using bucket \" + bucket)\n",
    "\n",
    "# send data to S3. SageMaker will take training data from s3\n",
    "sk_prefix = \"sagemaker/mobile_price_classification/sklearncontainer\"\n",
    "trainpath = sess.upload_data(\n",
    "    path=\"train-V-1.csv\", bucket=bucket, key_prefix=sk_prefix\n",
    ")\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path=\"test-V-1.csv\", bucket=bucket, key_prefix=sk_prefix\n",
    ")\n",
    "\n",
    "%%writefile script.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import sklearn\n",
    "import joblib\n",
    "import boto3\n",
    "import pathlib\n",
    "from io import StringIO \n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info(f\"Starting training\")\n",
    "\n",
    "    #data prepare\n",
    "    self.parameter_lr = self.param['parameter_lr']\n",
    "    self.penal,self.size= self.parameter_lr[\"penalty\"],self.parameter_lr[\"size\"]\n",
    "    self.parameter_lr.pop(\"penalty\")\n",
    "    self.parameter_lr.pop(\"size\")\n",
    "\n",
    "\n",
    "    def fit_ml_models(algo, algo_param, algo_name,x_train,y_train,x_test,y_test):\n",
    "    \n",
    "        algo = Pipeline([(\"algo\", algo)])\n",
    "        \n",
    "        \n",
    "        model = GridSearchCV(algo, param_grid=algo_param, cv=10, verbose=1)\n",
    "        \n",
    "        \n",
    "        logger.info(f\"Fitting {algo_name}\")\n",
    "        fit_model = model.fit(x_train, y_train)\n",
    "        \n",
    "        best_params = model.best_params_\n",
    "        logger.info(\"Best Parameters: \"+f\"{best_params}\")\n",
    "        \n",
    "        best_model = model.best_estimator_\n",
    "        best_estimator = model.best_estimator_._final_estimator\n",
    "        best_score = round(model.best_score_, 4)\n",
    "        logger.info(f\"Best Score: \"+\"{:.3f}\".format(best_score))\n",
    "        \n",
    "        y_pred_train = model.predict(x_train)\n",
    "        y_pred_test = model.predict(x_test)\n",
    "        \n",
    "        \n",
    "        acc_score_train = round(accuracy_score(y_pred_train, y_train)*100, 3)\n",
    "        acc_score_test = round(accuracy_score(y_pred_test, y_test)*100, 3)\n",
    "        logger.info(f\"Train and Test Accuracy Score for train {acc_score_train} and test {acc_score_test}\")\n",
    "        \n",
    "        logger.info(f\"Finishing training\")\n",
    "\n",
    "    return acc_score_train, acc_score_test, best_score, best_params\n",
    "\n",
    "    def run_train(self):\n",
    "        model_lr = LogisticRegression(penalty=self.penal, random_state=42)\n",
    "\n",
    "        acc_score_train, acc_score_test, best_score , best_params= fit_ml_models(model_lr, self.parameter_lr, \"Logistic Regression\",\n",
    "                                                                             self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "    \n",
    "        model_lr = LogisticRegression(solver=best_params[\"algo__solver\"], C=best_params[\"algo__C\"],penalty=self.penal, random_state=42)\n",
    "        model_lr.fit(self.x_train, self.y_train)\n",
    "\n",
    "\n",
    "    model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model,model_path)\n",
    "    print(\"Model persisted at \" + model_path)\n",
    "\n",
    "\n",
    "    #self.df = self.df.drop(\"Id\", axis=1)\n",
    "    x = self.df.drop([\"species\"], axis=1)\n",
    "    y = self.df[\"species\"]\n",
    "\n",
    "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=self.size, random_state=42)\n",
    "\n",
    "\n",
    "    print(\"[INFO] Extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--random_state\", type=int, default=0)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train-V-1.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test-V-1.csv\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"SKLearn Version: \", sklearn.__version__)\n",
    "    print(\"Joblib Version: \", joblib.__version__)\n",
    "\n",
    "    print(\"[INFO] Reading data\")\n",
    "    print()\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "    \n",
    "    features = list(train_df.columns)\n",
    "    label = features.pop(-1)\n",
    "    \n",
    "    print(\"Building training and testing datasets\")\n",
    "    print()\n",
    "    X_train = train_df[features]\n",
    "    X_test = test_df[features]\n",
    "    y_train = train_df[label]\n",
    "    y_test = test_df[label]\n",
    "\n",
    "    print('Column order: ')\n",
    "    print(features)\n",
    "    print()\n",
    "    \n",
    "    print(\"Label column is: \",label)\n",
    "    print()\n",
    "    \n",
    "    print(\"Data Shape: \")\n",
    "    print()\n",
    "    print(\"---- SHAPE OF TRAINING DATA (85%) ----\")\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print()\n",
    "    print(\"---- SHAPE OF TESTING DATA (15%) ----\")\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    print()\n",
    "    \n",
    "  \n",
    "    print(\"Training RandomForest Model.....\")\n",
    "    print()\n",
    "    model =  RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.random_state, verbose = 3,n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model,model_path)\n",
    "    print(\"Model persisted at \" + model_path)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test,y_pred_test)\n",
    "    test_rep = classification_report(y_test,y_pred_test)\n",
    "\n",
    "    print()\n",
    "    print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "    print()\n",
    "    print(\"Total Rows are: \", X_test.shape[0])\n",
    "    print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "    print('[TESTING] Testing Report: ')\n",
    "    print(test_rep)\n",
    "\n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"RF-custom-sklearn\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 100,\n",
    "        \"random_state\": 0,\n",
    "    },\n",
    "    use_spot_instances = True,\n",
    "    max_wait = 7200,\n",
    "    max_run = 3600\n",
    ")\n",
    "\n",
    "\n",
    "# launch training job, with asynchronous call\n",
    "sklearn_estimator.fit({\"train\": trainpath, \"test\": testpath}, wait=True)\n",
    "# sklearn_estimator.fit({\"train\": datapath}, wait=True)\n",
    "\n",
    "\n",
    "sklearn_estimator.latest_training_job.wait(logs=\"None\")\n",
    "artifact = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=sklearn_estimator.latest_training_job.name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(\"Model artifact persisted at \" + artifact)\n",
    "\n",
    "\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = \"Custom-sklearn-model-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model = SKLearnModel(\n",
    "    name =  model_name,\n",
    "    model_data=artifact,\n",
    "    role=get_execution_role(),\n",
    "    entry_point=\"script.py\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    ")\n",
    "endpoint_name = \"Custom-sklearn-model-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        CREATE DATABASE if not exists iris_db;\\n\\n        USE iris_db;\\n\\n        CREATE TABLE iris_dataset (\\n            sepal_length FLOAT,\\n            sepal_width FLOAT,\\n            petal_length FLOAT,\\n            petal_width FLOAT,\\n            species FLOAT\\n        );                                                        \\n\\n        CREATE TABLE predictions (\\n            id INT PRIMARY KEY AUTO_INCREMENT,\\n            sepal_length FLOAT,\\n            sepal_width FLOAT,\\n            petal_length FLOAT,\\n            petal_width FLOAT,\\n            pred_species FLOAT,\\n            time_pred DOUBLE\\n        );\\n        '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"CREATE DATABASE if not exists iris_db;\n",
    "USE iris_db;\n",
    "\n",
    "        CREATE TABLE iris_dataset (\n",
    "            sepal_length FLOAT,\n",
    "            sepal_width FLOAT,\n",
    "            petal_length FLOAT,\n",
    "            petal_width FLOAT,\n",
    "            species FLOAT\n",
    "        );                                                        \n",
    "\n",
    "        CREATE TABLE predictions (\n",
    "            id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "            sepal_length FLOAT,\n",
    "            sepal_width FLOAT,\n",
    "            petal_length FLOAT,\n",
    "            petal_width FLOAT,\n",
    "            pred_species FLOAT,\n",
    "            time_pred DOUBLE\n",
    "        );\n",
    "        \"\"\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.predict(testX[features][0:2].values.tolist()))\n",
    "\n",
    "sm_boto3.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"PRUEBA1.0\\ABICHALLENGE_LUIS-ROMERO\")\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"../\")\n",
    "# ! pip install -r requirements.txt\n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"RF-custom-sklearn\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 100,\n",
    "        \"random_state\": 0,\n",
    "    },\n",
    "    use_spot_instances = True,\n",
    "    max_wait = 7200,\n",
    "    max_run = 3600\n",
    ")\n",
    "\n",
    "\n",
    "# launch training job, with asynchronous call\n",
    "sklearn_estimator.fit({\"train\": trainpath, \"test\": testpath}, wait=True)\n",
    "# sklearn_estimator.fit({\"train\": datapath}, wait=True)\n",
    "\n",
    "\n",
    "sklearn_estimator.latest_training_job.wait(logs=\"None\")\n",
    "artifact = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=sklearn_estimator.latest_training_job.name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(\"Model artifact persisted at \" + artifact)\n",
    "\n",
    "\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = \"Custom-sklearn-model-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model = SKLearnModel(\n",
    "    name =  model_name,\n",
    "    model_data=artifact,\n",
    "    role=get_execution_role(),\n",
    "    entry_point=\"script.py\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-19 07:36:18,091: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-08-19 07:36:18,093: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-08-19 07:36:18,098: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-08-19 07:36:18,100: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-08-19 07:36:18,101: INFO: common: created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "from Classifier.constants import *\n",
    "from Classifier.utils.common import read_yaml, create_directories\n",
    "\n",
    "config_filepath = CONFIG_FILE_PATH\n",
    "params_filepath = PARAMS_FILE_PATH\n",
    "\n",
    "config = read_yaml(config_filepath)\n",
    "params = read_yaml(params_filepath)\n",
    "\n",
    "from Classifier.config.configuration import ConfigurationManager as CM\n",
    "config = CM().get_data_training_config()\n",
    "\n",
    "param = config[\"params\"]\n",
    "conf = config[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_dir': WindowsPath('model'),\n",
       " 'trained_model_path': WindowsPath('model/model.pkl')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _mysql_connector\n",
    "param={'parameter_lr': {'algo__solver': ['liblinear', 'newton-cg'],\n",
    "  'algo__C': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "  'penalty': 'l2',\n",
    "  'size': 0.2}}\n",
    "\n",
    "# conf={paths...}\n",
    "\n",
    "config={\"user\":\"iris1\",\n",
    "            \"password\":\"Xi25_PS6iww9os?z3\",\n",
    "            \"host\":\"iris-db-instance.c3kq6wgkc2hl.us-east-1.rds.amazonaws.com\"}\n",
    "\n",
    "\n",
    "access_point=\"iris-acces-siwgf7f9fy1wu965gq533ba3nfkscuse1a-s3alias\"\n",
    "\n",
    "\n",
    "\n",
    "connection = mysql.connector.connect(**config)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#developer // production\n",
    "\n",
    "\n",
    "#subnet privada\n",
    "# subnet-0481d2f1cb01b8236\n",
    "#db subnet\n",
    "#subnet-064f2feaf0c3d4c0a\n",
    "# Configuración de VPC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # Check Sklearn version\n",
    "sklearn.__version__\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "import datetime\n",
    "import time\n",
    "import tarfile\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "#s3://iris-bucket1926/iris-sagemaker/\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "bucket = 'iris-bucket1926' # Mention the created S3 bucket name here\n",
    "print(\"Using bucket \" + bucket)\n",
    "\n",
    "# send data to S3. SageMaker will take training data from s3\n",
    "# sk_prefix = \"sagemaker/mobile_price_classification/sklearncontainer\"\n",
    "\n",
    "#(Load teh data )\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import sklearn\n",
    "import joblib\n",
    "import boto3\n",
    "import pathlib\n",
    "from io import StringIO \n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"[INFO] Extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--random_state\", type=int, default=0)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train-V-1.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test-V-1.csv\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"SKLearn Version: \", sklearn.__version__)\n",
    "    print(\"Joblib Version: \", joblib.__version__)\n",
    "\n",
    "    print(\"[INFO] Reading data\")\n",
    "    print()\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "    \n",
    "    features = list(train_df.columns)\n",
    "    label = features.pop(-1)\n",
    "    \n",
    "    print(\"Building training and testing datasets\")\n",
    "    print()\n",
    "    X_train = train_df[features]\n",
    "    X_test = test_df[features]\n",
    "    y_train = train_df[label]\n",
    "    y_test = test_df[label]\n",
    "\n",
    "    print('Column order: ')\n",
    "    print(features)\n",
    "    print()\n",
    "    \n",
    "    print(\"Label column is: \",label)\n",
    "    print()\n",
    "    \n",
    "    print(\"Data Shape: \")\n",
    "    print()\n",
    "    print(\"---- SHAPE OF TRAINING DATA (85%) ----\")\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print()\n",
    "    print(\"---- SHAPE OF TESTING DATA (15%) ----\")\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    print()\n",
    "    \n",
    "  \n",
    "    print(\"Training RandomForest Model.....\")\n",
    "    print()\n",
    "    model =  RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.random_state, verbose = 3,n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model,model_path)\n",
    "    print(\"Model persisted at \" + model_path)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test,y_pred_test)\n",
    "    test_rep = classification_report(y_test,y_pred_test)\n",
    "\n",
    "    print()\n",
    "    print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "    print()\n",
    "    print(\"Total Rows are: \", X_test.shape[0])\n",
    "    print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "    print('[TESTING] Testing Report: ')\n",
    "    print(test_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "\n",
    "endpoint_name=\"Iris_endpoint\"\n",
    "# Especifica tu rol de IAM\n",
    "rolef = 'arn:aws:iam::123456789012:role/SageMakerRole'\n",
    "\n",
    "# Configuración de VPC\n",
    "vpc_configf = {\n",
    "    'Subnets': ['subnet-abc123', 'subnet-def456'],  # Tus subnets\n",
    "    'SecurityGroupIds': ['sg-0123456789abcdef0']  # Tus security groups\n",
    "}\n",
    "\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"main.py\",\n",
    "    role=rolef,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"Custom-iris-sklearn\",\n",
    "    use_spot_instances = True,\n",
    "    vpc_config=vpc_configf,\n",
    "    max_wait = 7200,\n",
    "    max_run = 3600\n",
    ")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "# launch training job, with asynchronous call\n",
    "sklearn_estimator.fit( wait=True)\n",
    "# sklearn_estimator.fit({\"train\": datapath}, wait=True)\n",
    "\n",
    "\n",
    "# sklearn_estimator.latest_training_job.wait(logs=\"None\")\n",
    "# artifact = sm_boto3.describe_training_job(\n",
    "#     TrainingJobName=sklearn_estimator.latest_training_job.name\n",
    "# )[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(\"Model artifact persisted at \" + artifact)\n",
    "\n",
    "\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = \"Custom-sklearn-model-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model = SKLearnModel(\n",
    "    name =  model_name,\n",
    "    model_data=artifact,\n",
    "    role=get_execution_role(),\n",
    "    entry_point=\"script.py\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "\n",
    "endpoint_name=\"Iris_endpoint\"\n",
    "# Especifica tu rol de IAM\n",
    "rolef = 'arn:aws:iam::123456789012:role/SageMakerRole'\n",
    "\n",
    "#subnet privada\n",
    "# subnet-0481d2f1cb01b8236\n",
    "#db subnet\n",
    "#subnet-064f2feaf0c3d4c0a\n",
    "# Configuración de VPC\n",
    "vpc_configf = {\n",
    "    'Subnets': ['subnet-abc123', 'subnet-def456'],  # Tus subnets\n",
    "    'SecurityGroupIds': ['sg-0123456789abcdef0']  # Tus security groups\n",
    "}\n",
    "\n",
    "\n",
    "# Definir el estimador y entrenar\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"main.py\",\n",
    "    role=rolef,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"Custom-iris-sklearn\",\n",
    "    use_spot_instances=True,\n",
    "    vpc_config=vpc_configf,\n",
    "    max_wait=7200,\n",
    "    max_run=3600\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "sklearn_estimator.fit(wait=True)\n",
    "\n",
    "# Recuperar la ubicación de los artefactos del modelo en S3\n",
    "model_data = sklearn_estimator.model_data\n",
    "\n",
    "# Crear un objeto SKLearnModel usando los artefactos del modelo en S3\n",
    "model = SKLearnModel(\n",
    "    model_data=model_data,\n",
    "    role=rolef,\n",
    "    entry_point=\"main.py\",  # Mismo script que usaste para entrenar\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    vpc_config=vpc_configf\n",
    ")\n",
    "\n",
    "# Desplegar el modelo como un endpoint\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
